{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AITA: Automated Instructure Teaching Assistant\n",
    "\n",
    "This notebook implements the AITA tool for extracting and analyzing student discussion entries from Canvas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Required Imports and Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import traceback\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from playwright.async_api import Page\n",
    "from dotenv import load_dotenv\n",
    "import asyncio\n",
    "import os\n",
    "import re\n",
    "from typing import List, Union\n",
    "import pandas as pd\n",
    "from langchain_core.messages import HumanMessage\n",
    "from colorama import init, Fore, Style\n",
    "\n",
    "# Assuming these are your imports from browser_use for the authenticator\n",
    "from browser_use import Agent, Controller \n",
    "from browser_use.browser.browser import Browser, BrowserConfig\n",
    "from playwright.async_api import Page, FrameLocator, Locator\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration and Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "OUTPUT_FOLDER_NAME = \"student_submissions_output\"\n",
    "\n",
    "# Initialize colorama\n",
    "init(autoreset=True)\n",
    "\n",
    "# Authentication task instructions\n",
    "AUTH_TASK = \"\"\"\n",
    "            - Open the URL: {url}\n",
    "            - Authenticate using the following credentials:\n",
    "            - Enter email: {ms_email}\n",
    "            - Enter password: {ms_password}\n",
    "            - Wait for User to approve login on Authenticator app. Do not proceed until approved.\n",
    "            - Ensure you land on the SpeedGrader page for the first student.\n",
    "        \"\"\"\n",
    "\n",
    "# Sensitive data (Get from environment variables)\n",
    "sensitive_data = {\n",
    "    \"ms_email\": os.getenv(\"MS_EMAIL\", \"A02458093@aggies.usu.edu\"), # Example: get from env or default\n",
    "    \"ms_password\": os.getenv(\"MS_PASSWORD\", \"4Future$100%!\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Logger Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define colored logging functions\n",
    "def log_info(message): print(f\"{Fore.CYAN}[INFO] {message}{Style.RESET_ALL}\")\n",
    "def log_success(message): print(f\"{Fore.GREEN}[SUCCESS] {message}{Style.RESET_ALL}\")\n",
    "def log_warning(message): print(f\"{Fore.YELLOW}[WARNING] {message}{Style.RESET_ALL}\")\n",
    "def log_error(message): print(f\"{Fore.RED}[ERROR] {message}{Style.RESET_ALL}\")\n",
    "def log_debug(message): print(f\"{Fore.MAGENTA}[DEBUG] {message}{Style.RESET_ALL}\")\n",
    "def log_step(step_num, message): print(f\"{Fore.BLUE}[STEP {step_num}] {message}{Style.RESET_ALL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanitize filename\n",
    "def sanitize_filename(name: str) -> str:\n",
    "    name = re.sub(r'[^\\w\\s-]', '', name) # Remove invalid chars\n",
    "    name = re.sub(r'\\s+', '_', name).strip('_') # Replace spaces with underscores\n",
    "    return name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Optional, Dict\n",
    "\n",
    "class DiscussionEntry(BaseModel):\n",
    "    author: Optional[str] = Field(default=\"Author not found\")\n",
    "    post_date: Optional[str] = Field(default=\"Date not found\")\n",
    "    content: Optional[str] = Field(default=\"Content not found\")\n",
    "\n",
    "class StudentSubmissionData(BaseModel):\n",
    "    student_id: Optional[str] = Field(default=\"ID not found\")\n",
    "    student_name: Optional[str] = Field(default=\"Name not found\")\n",
    "    entries: List[DiscussionEntry] = []\n",
    "    status: Optional[str] = None\n",
    "    error: Optional[str] = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Data Extraction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def extract_data_for_current_student(page: Page) -> StudentSubmissionData:\n",
    "    log_info(f\"Attempting to extract data for student at URL: {page.url}\")\n",
    "    current_student_id = \"ID not found\"\n",
    "    current_student_name = \"Name not found\"\n",
    "\n",
    "    try:\n",
    "        # Steps 1 & 2: Student ID and Name extraction (Using the slightly improved logic from the last iteration for robustness)\n",
    "        url = page.url\n",
    "        student_id_match = re.search(r'student_id=(\\d+)', url)\n",
    "        if student_id_match: current_student_id = student_id_match.group(1)\n",
    "        \n",
    "        if current_student_id != \"ID not found\": log_success(f\"Student ID: {current_student_id}\")\n",
    "        else: log_warning(\"Student ID not found in URL.\")\n",
    "\n",
    "        name_selectors = [\"span.ui-selectmenu-status span.ui-selectmenu-item-header\", \"#speedgrader_selected_student_label\"]\n",
    "        for selector in name_selectors:\n",
    "            name_element_locator = page.locator(selector).first\n",
    "            if await name_element_locator.count() > 0:\n",
    "                try:\n",
    "                    # Using is_visible with a timeout from the old robust version, but on name_element_locator directly\n",
    "                    if await name_element_locator.is_visible(timeout=1000): # Short timeout for name\n",
    "                        raw_name = await name_element_locator.text_content()\n",
    "                        if raw_name:\n",
    "                            current_student_name = re.split(r'\\(ID:|\\sAttempt\\s\\d', raw_name)[0].strip()\n",
    "                            break\n",
    "                    else:\n",
    "                        log_debug(f\"Name element for selector '{selector}' not visible quickly.\")\n",
    "                except Exception as e_name_vis:\n",
    "                    log_debug(f\"Error checking visibility for name selector '{selector}': {e_name_vis}\")\n",
    "            \n",
    "        if current_student_name != \"Name not found\": log_success(f\"Student Name: {current_student_name}\")\n",
    "        else: log_warning(f\"Student Name not found for ID {current_student_id}.\")\n",
    "\n",
    "        # --- \"No Submission\" Check (from the last reliable iteration) ---\n",
    "        no_submission_indicator_sel = \"div#this_student_does_not_have_a_submission\"\n",
    "        no_submission_indicator = page.locator(no_submission_indicator_sel)\n",
    "        \n",
    "        try:\n",
    "            await no_submission_indicator.wait_for(state=\"visible\", timeout=2000)\n",
    "            log_info(f\"Student {current_student_id} ({current_student_name}): Confirmed no submission via indicator '{no_submission_indicator_sel}'.\")\n",
    "            return StudentSubmissionData(\n",
    "                student_id=current_student_id,\n",
    "                student_name=current_student_name,\n",
    "                entries=[],\n",
    "                status=\"This student does not have a submission for this assignment (explicit indicator).\"\n",
    "            )\n",
    "        except Exception: \n",
    "            log_info(f\"Indicator '{no_submission_indicator_sel}' not visible. Assuming a submission exists for {current_student_id} ({current_student_name}).\")\n",
    "        # --- End \"No Submission\" Check ---\n",
    "\n",
    "        # 3. IFRAME DETECTION AND SCOPE SWITCHING (from the last reliable iteration, slightly adjusted for clarity)\n",
    "        submission_scope: Union[Page, FrameLocator] = page # Default to main page\n",
    "        iframe_focused = False\n",
    "        log_info(f\"Initial submission_scope is main page ({page.url}).\")\n",
    "        \n",
    "        iframe_holder_locator = page.locator('div#iframe_holder')\n",
    "        iframe_sel_to_check = 'iframe#speedgrader_iframe'\n",
    "        \n",
    "        try:\n",
    "            await iframe_holder_locator.wait_for(state=\"visible\", timeout=5000) # Check if iframe container is visible\n",
    "            log_debug(f\"iframe_holder 'div#iframe_holder' is visible for {current_student_id}.\")\n",
    "            \n",
    "            iframe_locator_on_page = page.locator(iframe_sel_to_check) # Now locate the iframe\n",
    "            if await iframe_locator_on_page.count() > 0:\n",
    "                log_info(f\"Iframe element(s) FOUND for selector '{iframe_sel_to_check}'. Using first.\")\n",
    "                iframe_element = iframe_locator_on_page.first\n",
    "                try:\n",
    "                    await iframe_element.wait_for(state=\"visible\", timeout=5000) # Wait for iframe itself to be visible\n",
    "                    current_frame_scope = iframe_element.frame_locator(':scope')\n",
    "                    # Wait for body inside iframe to ensure content is loaded and visible\n",
    "                    await current_frame_scope.locator('body').wait_for(state=\"visible\", timeout=10000) \n",
    "                    \n",
    "                    log_success(f\"Successfully focused on iframe '{iframe_sel_to_check}'. New submission_scope is this FrameLocator.\")\n",
    "                    submission_scope = current_frame_scope # Switch scope to the iframe\n",
    "                    iframe_focused = True\n",
    "                except Exception as e_iframe_focus:\n",
    "                    log_warning(f\"Error focusing/interacting with iframe '{iframe_sel_to_check}': {e_iframe_focus}.\")\n",
    "            else:\n",
    "                log_warning(f\"iframe_holder was visible, but iframe '{iframe_sel_to_check}' count was 0.\")\n",
    "        except Exception as e_iframe_holder:\n",
    "            log_warning(f\"iframe_holder 'div#iframe_holder' was NOT visible or error: {e_iframe_holder}. Assuming content (if any) is on main page.\")\n",
    "        \n",
    "        if not iframe_focused:\n",
    "            log_warning(f\"No specific iframe focused for {current_student_id}. Content search will be on main page.\")\n",
    "        \n",
    "        # --- SECTION 4: LOCATE THE MAIN CONTENT AREA (FROM OLD ROBUST FUNCTION) ---\n",
    "        log_info(f\"Current submission_scope for content search: {type(submission_scope)}\")\n",
    "        main_content_container_sel = 'div#content.ic-Layout-contentMain'\n",
    "        submission_description_sel = 'div.submission_description'\n",
    "        search_root_locator: Union[Page, FrameLocator, Locator] = submission_scope # Default\n",
    "        \n",
    "        # Note: .first is a property, not a method call\n",
    "        main_content_loc = submission_scope.locator(main_content_container_sel).first \n",
    "        \n",
    "        # Using the old robust logic: count > 0 and is_visible(timeout=...)\n",
    "        if await main_content_loc.count() > 0 and await main_content_loc.is_visible(timeout=7000):\n",
    "            log_success(f\"'{main_content_container_sel}' is VISIBLE in current scope.\")\n",
    "            submission_desc_loc = main_content_loc.locator(submission_description_sel).first\n",
    "            if await submission_desc_loc.count() > 0 and await submission_desc_loc.is_visible(timeout=5000):\n",
    "                log_success(f\"'{submission_description_sel}' is VISIBLE. Using it as search_root_locator.\")\n",
    "                search_root_locator = submission_desc_loc\n",
    "            else:\n",
    "                log_warning(f\"'{submission_description_sel}' not found/visible within '{main_content_container_sel}'. Using parent '{main_content_container_sel}' as search_root.\")\n",
    "                search_root_locator = main_content_loc\n",
    "        else:\n",
    "            log_warning(f\"'{main_content_container_sel}' NOT FOUND or NOT VISIBLE in current scope ({type(submission_scope)}). Search root will be the full submission_scope.\")\n",
    "            # search_root_locator remains submission_scope if main_content_loc is not found/visible\n",
    "\n",
    "        log_info(f\"Final search_root_locator type: {type(search_root_locator)}\")\n",
    "\n",
    "        # --- SECTION 5: FIND ALL DISCUSSION ENTRIES (FROM OLD ROBUST FUNCTION) ---\n",
    "        entry_selector = 'div.discussion_entry.communication_message'\n",
    "        entry_selector_fallback = 'article.discussion-entry, div.comment_holder > div.comment'\n",
    "        \n",
    "        # search_root_locator can be Page, FrameLocator, or Locator. All have .locator()\n",
    "        discussion_entry_locators = await search_root_locator.locator(entry_selector).all()\n",
    "        if not discussion_entry_locators:\n",
    "            log_debug(f\"No entries found with primary selector '{entry_selector}'. Trying fallback...\")\n",
    "            discussion_entry_locators = await search_root_locator.locator(entry_selector_fallback).all()\n",
    "\n",
    "        if not discussion_entry_locators:\n",
    "            status_msg = f\"No discussion entry elements found in the determined content area ({type(search_root_locator)}) for student {current_student_id}.\"\n",
    "            if iframe_focused: # Add specificity if iframe was the target\n",
    "                 status_msg = f\"Submission iframe was focused, but no discussion entries found within it (search root: {type(search_root_locator)}) for student {current_student_id}.\"\n",
    "            log_warning(status_msg)\n",
    "            return StudentSubmissionData(student_id=current_student_id, student_name=current_student_name, entries=[], status=status_msg)\n",
    "\n",
    "        log_success(f\"Found {len(discussion_entry_locators)} potential discussion entry elements. Starting parsing loop...\")\n",
    "        extracted_entries: List[DiscussionEntry] = []\n",
    "\n",
    "        # --- DETAILED PARSING LOOP (FROM OLD ROBUST FUNCTION) ---\n",
    "        for i, entry_loc_item in enumerate(discussion_entry_locators): \n",
    "            log_step(i + 1, f\"Processing entry element {i + 1}/{len(discussion_entry_locators)}\")\n",
    "            try:\n",
    "                # Ensure element is attached before trying to get HTML. Short timeout.\n",
    "                await entry_loc_item.wait_for(state=\"attached\", timeout=2000) \n",
    "                entry_html_snippet_handle = await entry_loc_item.evaluate_handle('(element) => element.outerHTML.slice(0, 1200)')\n",
    "                log_debug(f\"Entry {i+1} HTML SNIPPET:\\n{await entry_html_snippet_handle.json_value()}\")\n",
    "            except Exception as e_entry_html: \n",
    "                log_warning(f\"Could not get HTML snippet for entry {i+1}: {e_entry_html}\")\n",
    "\n",
    "            author_entry = current_student_name if current_student_name != \"Name not found\" else \"Student name not resolved\"\n",
    "            log_debug(f\"Entry {i+1}: Author determined as '{author_entry}'\")\n",
    "\n",
    "            post_date_entry = \"Date not found\"\n",
    "            content_entry = \"Content not found\"\n",
    "\n",
    "            # Date Extraction (Old Robust Logic)\n",
    "            date_sels_map = {\n",
    "                'div.header div.post_date.time_ago_date': ['data-timestamp', 'title', 'text'],\n",
    "                '.discussion-header-content time': ['datetime', 'title', 'text'],\n",
    "                '.posted_at time': ['datetime', 'title', 'text'],\n",
    "            }\n",
    "            date_found_for_this_entry = False\n",
    "            for date_sel_str_item, attr_priority_item in date_sels_map.items():\n",
    "                if date_found_for_this_entry: break\n",
    "                log_debug(f\"Entry {i+1}: Trying date selector '{date_sel_str_item}'\")\n",
    "                date_element_loc = entry_loc_item.locator(date_sel_str_item).first # .first is a property\n",
    "                \n",
    "                if await date_element_loc.count() > 0: \n",
    "                    log_debug(f\"Entry {i+1}:   FOUND element for date selector '{date_sel_str_item}'.\")\n",
    "                    try:\n",
    "                        # No explicit wait_for here, as per old robust logic (relying on count > 0)\n",
    "                        for attr_type in attr_priority_item:\n",
    "                            val = None\n",
    "                            if attr_type == 'text':\n",
    "                                val = await date_element_loc.text_content()\n",
    "                                log_debug(f\"Entry {i+1}:     '{date_sel_str_item}' -> text_content(): '{val}'\")\n",
    "                            else:\n",
    "                                val = await date_element_loc.get_attribute(attr_type)\n",
    "                                log_debug(f\"Entry {i+1}:     '{date_sel_str_item}' -> get_attribute('{attr_type}'): '{val}'\")\n",
    "                            \n",
    "                            if val and val.strip():\n",
    "                                post_date_entry = val.strip()\n",
    "                                log_success(f\"Entry {i+1}:   DATE extracted as '{post_date_entry}' using selector '{date_sel_str_item}' (from '{attr_type}').\")\n",
    "                                date_found_for_this_entry = True; break \n",
    "                        if date_found_for_this_entry: break \n",
    "                    except Exception as e_date_extract:\n",
    "                        log_warning(f\"Entry {i+1}:     Error processing date element for '{date_sel_str_item}': {e_date_extract}\")\n",
    "                else:\n",
    "                    log_debug(f\"Entry {i+1}:   NO element found for date selector '{date_sel_str_item}'.\")\n",
    "            if not date_found_for_this_entry: log_warning(f\"Entry {i+1}: DATE extraction FAILED.\")\n",
    "\n",
    "            # Content Extraction (Old Robust Logic - using text_content())\n",
    "            content_sels_list = [\n",
    "                'div.content div.message.user_content.enhanced', '.message_body', '.entry_content'\n",
    "            ]\n",
    "            content_found_for_this_entry = False\n",
    "            for content_sel_str_item in content_sels_list:\n",
    "                if content_found_for_this_entry: break\n",
    "                log_debug(f\"Entry {i+1}: Trying content selector '{content_sel_str_item}'\")\n",
    "                content_element_loc = entry_loc_item.locator(content_sel_str_item).first # .first is a property\n",
    "\n",
    "                if await content_element_loc.count() > 0:\n",
    "                    log_debug(f\"Entry {i+1}:   FOUND element for content selector '{content_sel_str_item}'.\")\n",
    "                    try:\n",
    "                        # Using text_content() as per old robust logic\n",
    "                        extracted_text = await content_element_loc.text_content() \n",
    "                        log_debug(f\"Entry {i+1}:     Raw text (len {len(extracted_text or '')}): '{(extracted_text or '')[:200]}...'\")\n",
    "                        if extracted_text and extracted_text.strip():\n",
    "                            content_entry = extracted_text.strip()\n",
    "                            log_success(f\"Entry {i+1}:   CONTENT extracted (len {len(content_entry)}) using '{content_sel_str_item}'.\")\n",
    "                            content_found_for_this_entry = True; break\n",
    "                        else: log_debug(f\"Entry {i+1}:     Content element found but text is empty/whitespace.\")\n",
    "                    except Exception as e_content_extract:\n",
    "                        log_warning(f\"Entry {i+1}:     Error processing content element for '{content_sel_str_item}': {e_content_extract}\")\n",
    "                else:\n",
    "                    log_debug(f\"Entry {i+1}:   NO element found for content selector '{content_sel_str_item}'.\")\n",
    "            if not content_found_for_this_entry: log_warning(f\"Entry {i+1}: CONTENT extraction FAILED.\")\n",
    "            \n",
    "            if post_date_entry != \"Date not found\" or content_entry != \"Content not found\":\n",
    "                extracted_entries.append(DiscussionEntry(author=author_entry, post_date=post_date_entry, content=content_entry))\n",
    "                log_success(f\"Entry {i+1}: ADDED to extracted_entries list.\")\n",
    "            else:\n",
    "                log_error(f\"Entry {i+1}: SKIPPED. No valid date OR content found for student {current_student_id}.\")\n",
    "        # --- END OF DETAILED PARSING LOOP ---\n",
    "\n",
    "        # Final status reporting (using old robust logic's style for this path)\n",
    "        if not extracted_entries and discussion_entry_locators: \n",
    "            status_msg = f\"Found {len(discussion_entry_locators)} entry elements for student {current_student_id}, but NO meaningful data could be extracted.\"\n",
    "            log_error(status_msg)\n",
    "            return StudentSubmissionData(student_id=current_student_id, student_name=current_student_name, entries=[], status=status_msg)\n",
    "        \n",
    "        final_status_message = f\"Successfully extracted {len(extracted_entries)} entries for student {current_student_id}.\"\n",
    "        if iframe_focused:\n",
    "            final_status_message += \" (from iframe)\"\n",
    "        else:\n",
    "            final_status_message += \" (from main page content)\" # Simplified if not iframe\n",
    "\n",
    "        log_success(final_status_message)\n",
    "        return StudentSubmissionData(\n",
    "            student_id=current_student_id, \n",
    "            student_name=current_student_name, \n",
    "            entries=extracted_entries, \n",
    "            status=final_status_message\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        error_msg = f\"CRITICAL Error during extraction for student {current_student_id} ({current_student_name}): {str(e)}\"\n",
    "        log_error(error_msg)\n",
    "        traceback.print_exc()\n",
    "        return StudentSubmissionData(student_id=current_student_id, student_name=current_student_name, status=\"Extraction failed with critical error.\", error=str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Submission Analyzer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SubmissionAnalyzer:\n",
    "    def __init__(self, llm_instance: ChatGoogleGenerativeAI, max_entries: int = 4):\n",
    "        self.llm = llm_instance\n",
    "        self.max_entries = max_entries\n",
    "        log_info(f\"SubmissionAnalyzer initialized with LLM: {llm_instance.model} and max_entries: {max_entries}\")\n",
    "\n",
    "    async def _get_summary(self, content: str) -> str:\n",
    "        if not content or content == \"Content not found\":\n",
    "            return \"No content to summarize\"\n",
    "        try:\n",
    "            prompt = f\"Please summarize the following student discussion entry in one or two sentences:\\n\\n---\\n{content}\\n---\\n\\nSummary:\"\n",
    "            log_debug(f\"Attempting to summarize content (first 100 chars): {content[:100]}...\")\n",
    "            # Assuming your LLM has an `ainvoke` method for async calls\n",
    "            # and accepts a string or a list of messages\n",
    "            response = await self.llm.ainvoke([HumanMessage(content=prompt)])\n",
    "            summary = response.content.strip()\n",
    "            log_success(f\"Summary generated (first 50 chars): {summary[:50]}...\")\n",
    "            return summary\n",
    "        except Exception as e:\n",
    "            log_error(f\"Error generating summary: {e}\")\n",
    "            traceback.print_exc()\n",
    "            return \"Error generating summary\"\n",
    "\n",
    "    async def process_json_report(self, json_file_path: str, output_csv_path: str):\n",
    "        log_info(f\"Starting processing of JSON report: {json_file_path}\")\n",
    "        try:\n",
    "            with open(json_file_path, 'r', encoding='utf-8') as f:\n",
    "                all_students_data_raw = json.load(f)\n",
    "            log_success(f\"Successfully loaded data for {len(all_students_data_raw)} students from {json_file_path}\")\n",
    "        except FileNotFoundError:\n",
    "            log_error(f\"JSON report file not found: {json_file_path}\")\n",
    "            return\n",
    "        except json.JSONDecodeError:\n",
    "            log_error(f\"Error decoding JSON from file: {json_file_path}\")\n",
    "            return\n",
    "        except Exception as e:\n",
    "            log_error(f\"An unexpected error occurred loading JSON file: {e}\")\n",
    "            return\n",
    "\n",
    "        # Validate data structure (optional, but good practice)\n",
    "        if not isinstance(all_students_data_raw, list):\n",
    "            log_error(\"JSON data is not a list as expected.\")\n",
    "            return\n",
    "\n",
    "        all_students_data = [StudentSubmissionData(**data) for data in all_students_data_raw]\n",
    "\n",
    "        processed_rows = []\n",
    "        summary_tasks = [] # For collecting summarization tasks to run concurrently\n",
    "\n",
    "        # First pass: Prepare data and create summarization tasks\n",
    "        for i, student_data in enumerate(all_students_data): # student_data is a StudentSubmissionData instance\n",
    "            log_step(i + 1, f\"Processing student: {student_data.student_name} (ID: {student_data.student_id})\")\n",
    "            # ADD THIS:\n",
    "            log_debug(f\"Student {student_data.student_id} has {len(student_data.entries)} entries (according to Pydantic).\")\n",
    "            if not student_data.entries:\n",
    "                log_debug(f\"  No entries found for student {student_data.student_id} in the parsed data.\")\n",
    "\n",
    "            row = { # This initialization is fine\n",
    "                \"student_id\": student_data.student_id,\n",
    "                \"student_name\": student_data.student_name,\n",
    "            }\n",
    "            for entry_num in range(1, self.max_entries + 1):\n",
    "                row[f\"entry_{entry_num}_date\"] = None\n",
    "                row[f\"entry_{entry_num}_content\"] = None\n",
    "                row[f\"entry_{entry_num}_summary\"] = None\n",
    "\n",
    "            for j, entry in enumerate(student_data.entries): # entry is a DiscussionEntry instance\n",
    "                if j < self.max_entries:\n",
    "                    entry_idx = j + 1\n",
    "                    # ADD/MODIFY THIS DEBUG LINE:\n",
    "                    log_debug(f\"  Student {student_data.student_id}, Parsed Entry {entry_idx}/{len(student_data.entries)}: \"\n",
    "                              f\"post_date='{entry.post_date}' (Type: {type(entry.post_date)}), \"\n",
    "                              f\"content_preview='{(entry.content or '')[:100]}...' (Type: {type(entry.content)})\")\n",
    "\n",
    "                    row[f\"entry_{entry_idx}_date\"] = entry.post_date\n",
    "                    row[f\"entry_{entry_idx}_content\"] = entry.content\n",
    "\n",
    "                    if entry.content and entry.content != \"Content not found\":\n",
    "                        log_debug(f\"    Content for entry {entry_idx} IS valid for summarization.\")\n",
    "                        summary_tasks.append(\n",
    "                            (self._get_summary(entry.content), row, f\"entry_{entry_idx}_summary\")\n",
    "                        )\n",
    "                    else:\n",
    "                        # ADD/MODIFY THIS WARNING:\n",
    "                        log_warning(f\"    Student {student_data.student_id}, Entry {entry_idx}: Content NOT valid for summarization. \"\n",
    "                                    f\"Actual content value: '{entry.content}' (Type: {type(entry.content)}). \"\n",
    "                                    f\"Is None: {entry.content is None}, Is empty string: {entry.content == ''}, Is 'Content not found': {entry.content == 'Content not found'}\")\n",
    "                        # Ensure a placeholder summary if no summary is generated\n",
    "                        row[f\"entry_{entry_idx}_summary\"] = \"Content unsuitable for summary\" # Or leave as None\n",
    "                else:\n",
    "                    log_warning(f\"Student {student_data.student_id} has more than {self.max_entries} entries. Skipping extras for CSV.\")\n",
    "                    break\n",
    "            processed_rows.append(row)\n",
    "\n",
    "        # Second pass: Execute all summarization tasks concurrently\n",
    "        log_info(f\"Starting generation of {len(summary_tasks)} summaries...\")\n",
    "        summary_results_with_refs = await asyncio.gather(*(task for task, _, _ in summary_tasks))\n",
    "        log_success(f\"Completed {len(summary_results_with_refs)} summary generations.\")\n",
    "\n",
    "        # Third pass: Populate summaries back into the rows\n",
    "        for i, (task_tuple) in enumerate(summary_tasks):\n",
    "            _, row_ref, summary_key_ref = task_tuple\n",
    "            row_ref[summary_key_ref] = summary_results_with_refs[i]\n",
    "\n",
    "        # Create DataFrame and save to CSV\n",
    "        if not processed_rows:\n",
    "            log_warning(\"No data processed to write to CSV.\")\n",
    "            return\n",
    "\n",
    "        try:\n",
    "            df = pd.DataFrame(processed_rows)\n",
    "            # Define column order explicitly\n",
    "            columns = [\"student_id\", \"student_name\"]\n",
    "            for i in range(1, self.max_entries + 1):\n",
    "                columns.extend([f\"entry_{i}_date\", f\"entry_{i}_content\", f\"entry_{i}_summary\"])\n",
    "            \n",
    "            # Ensure all expected columns exist, adding them if they were missed (e.g. if no student had 4 entries)\n",
    "            for col in columns:\n",
    "                if col not in df.columns:\n",
    "                    df[col] = None # Or pd.NA or \"\"\n",
    "\n",
    "            df = df[columns] # Reorder/select columns\n",
    "\n",
    "            df.to_csv(output_csv_path, index=False, encoding='utf-8')\n",
    "            log_success(f\"Successfully wrote processed data to CSV: {output_csv_path}\")\n",
    "        except Exception as e:\n",
    "            log_error(f\"Error writing data to CSV: {e}\")\n",
    "            traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Analysis Runner Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_submission_analysis(llm_instance: ChatGoogleGenerativeAI):\n",
    "    \"\"\"\n",
    "    Function to run the submission analysis part.\n",
    "    \"\"\"\n",
    "    log_info(\"Starting submission analysis process...\")\n",
    "    analyzer = SubmissionAnalyzer(llm_instance=llm_instance, max_entries=4)\n",
    "\n",
    "    json_report_path = os.path.join(OUTPUT_FOLDER_NAME, \"ALL_students_compiled_report.json\")\n",
    "    csv_output_path = os.path.join(OUTPUT_FOLDER_NAME, \"analyzed_student_submissions.csv\")\n",
    "\n",
    "    if not os.path.exists(json_report_path):\n",
    "        log_error(f\"Cannot perform analysis: Compiled JSON report '{json_report_path}' not found.\")\n",
    "        log_warning(\"Please ensure the main data extraction script (main function) runs successfully first.\")\n",
    "        return\n",
    "\n",
    "    await analyzer.process_json_report(json_file_path=json_report_path, output_csv_path=csv_output_path)\n",
    "    log_info(\"Submission analysis process finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize browser and LLM\n",
    "browser_manager = Browser()\n",
    "model = ChatGoogleGenerativeAI(model='gemini-2.0-flash-lite') # Keep for authenticator\n",
    "\n",
    "async def main():\n",
    "    all_students_data: List[StudentSubmissionData] = []\n",
    "    \n",
    "    if not os.path.exists(OUTPUT_FOLDER_NAME):\n",
    "        os.makedirs(OUTPUT_FOLDER_NAME)\n",
    "        log_info(f\"Created output folder: ./{OUTPUT_FOLDER_NAME}/\")\n",
    "\n",
    "    async with await browser_manager.new_context() as context: # This is browser_use.BrowserContext\n",
    "        authenticator_agent = Agent(\n",
    "            task=AUTH_TASK.format(\n",
    "                url=\"https://usu.instructure.com/courses/780705/gradebook/speed_grader?assignment_id=4809230&student_id=1812493\",\n",
    "                ms_email=\"ms_email\",\n",
    "                ms_password=\"ms_password\",\n",
    "            ),\n",
    "            llm=model, # If your agent uses an LLM\n",
    "            message_context=\"You are a browser automation agent for login.\",\n",
    "            browser_context=context,\n",
    "            sensitive_data=sensitive_data,\n",
    "            # controller=controller, # If agent uses registered actions\n",
    "        )\n",
    "        try:\n",
    "            await authenticator_agent.run()\n",
    "            log_success(\"Authentication and navigation to SpeedGrader complete.\")\n",
    "        except Exception as auth_err:\n",
    "            log_error(f\"Authenticator agent failed: {auth_err}\")\n",
    "            traceback.print_exc()\n",
    "            await browser_manager.close()\n",
    "            return\n",
    "\n",
    "        page = await context.get_current_page()\n",
    "        if not page:\n",
    "            log_error(\"Failed to get current page from browser_context after authentication.\")\n",
    "            await browser_manager.close()\n",
    "            return\n",
    "\n",
    "        log_info(\"Starting Playwright data extraction loop...\")\n",
    "        next_button_selector = \"button#next-student-button, button[aria-label='Next Student'], button[data-testid='next-student-button']\"\n",
    "        # prev_button_selector = \"button#prev-student-button, button[aria-label='Previous Student']\" # For reference\n",
    "        \n",
    "        processed_student_ids_this_run = set()\n",
    "        # MAX_STUDENTS = 3 # For testing, uncomment and set a small number\n",
    "        # students_done_count = 0\n",
    "\n",
    "        while True: # students_done_count < MAX_STUDENTS:\n",
    "            current_url_for_check = page.url # For checking if URL changes after click\n",
    "            \n",
    "            log_info(f\"Processing page: {current_url_for_check}\")\n",
    "\n",
    "            # Wait for page to stabilize (next button usable, network idle)\n",
    "            try:\n",
    "                next_button_loc_check = page.locator(next_button_selector).first\n",
    "                await next_button_loc_check.wait_for(state=\"visible\", timeout=15000)\n",
    "                log_debug(\"Next button visible. Waiting for network idle...\")\n",
    "                await page.wait_for_load_state('networkidle', timeout=30000) # Increased timeout\n",
    "                log_debug(\"Page network idle.\")\n",
    "            except Exception as e_wait_stable:\n",
    "                log_error(f\"Page did not stabilize for student at {current_url_for_check}: {e_wait_stable}\")\n",
    "                # Decide: break or try to extract? For now, try to extract.\n",
    "                log_warning(\"Attempting extraction despite potential page instability.\")\n",
    "\n",
    "            student_data = await extract_data_for_current_student(page)\n",
    "            \n",
    "            # Check for loop conditions or inability to get ID\n",
    "            if student_data.student_id == \"ID not found\":\n",
    "                log_error(\"Student ID could not be determined. Breaking loop to prevent processing unknown student.\")\n",
    "                all_students_data.append(student_data) # Save what we have\n",
    "                break\n",
    "            if student_data.student_id in processed_student_ids_this_run:\n",
    "                log_warning(f\"Student ID {student_data.student_id} re-encountered. This could mean the page didn't advance. Breaking loop.\")\n",
    "                all_students_data.append(student_data) # Save what we have before breaking\n",
    "                break\n",
    "            \n",
    "            processed_student_ids_this_run.add(student_data.student_id)\n",
    "            all_students_data.append(student_data)\n",
    "            # students_done_count += 1\n",
    "\n",
    "            # Save individual student data\n",
    "            s_id = sanitize_filename(student_data.student_id)\n",
    "            s_name = sanitize_filename(student_data.student_name if student_data.student_name != \"Name not found\" else \"UnknownName\")\n",
    "            individual_filename = os.path.join(OUTPUT_FOLDER_NAME, f\"student_{s_id}_{s_name}.json\")\n",
    "            try:\n",
    "                with open(individual_filename, \"w\", encoding='utf-8') as f_out:\n",
    "                    json.dump(student_data.model_dump(), f_out, indent=2, ensure_ascii=False)\n",
    "                log_success(f\"Saved data for {s_name} ({s_id}) to {individual_filename}\")\n",
    "            except Exception as e_save_ind:\n",
    "                log_error(f\"Failed to save individual file {individual_filename}: {e_save_ind}\")\n",
    "\n",
    "            # Navigate to the next student\n",
    "            next_button_locator = page.locator(next_button_selector).first\n",
    "            if not await next_button_locator.is_visible(timeout=5000) or not await next_button_locator.is_enabled(timeout=5000):\n",
    "                log_info(\"Next student button is not visible or enabled. Assuming end of student list.\")\n",
    "                break\n",
    "            \n",
    "            log_info(\"Clicking 'Next Student' button...\")\n",
    "            try:\n",
    "                await next_button_locator.click(timeout=10000)\n",
    "                # Wait for URL to change AND network to be idle (more robust)\n",
    "                log_debug(f\"Waiting for URL to change from {current_url_for_check} and network to settle...\")\n",
    "                await page.wait_for_function(\n",
    "                    f\"() => window.location.href !== '{current_url_for_check}' && window.location.href.includes('student_id=')\",\n",
    "                    timeout=20000 # Wait for URL to change\n",
    "                )\n",
    "                log_success(f\"URL changed to: {page.url}\")\n",
    "                await page.wait_for_load_state('networkidle', timeout=30000) # Then wait for content\n",
    "                log_success(\"Network idle after advancing to next student.\")\n",
    "            except Exception as e_nav:\n",
    "                log_error(f\"Error clicking 'Next Student' or waiting for new page: {e_nav}\")\n",
    "                if page.url == current_url_for_check:\n",
    "                     log_error(\"URL did not change. Potential stuck page. Breaking loop.\")\n",
    "                traceback.print_exc()\n",
    "                break\n",
    "        \n",
    "        log_success(f\"Finished iterating. Processed {len(all_students_data)} student records.\")\n",
    "\n",
    "        # Save compiled report\n",
    "        if all_students_data:\n",
    "            compiled_report_path = os.path.join(OUTPUT_FOLDER_NAME, \"ALL_students_compiled_report.json\")\n",
    "            try:\n",
    "                data_to_save = [s.model_dump() for s in all_students_data]\n",
    "                with open(compiled_report_path, \"w\", encoding='utf-8') as f_all:\n",
    "                    json.dump(data_to_save, f_all, indent=2, ensure_ascii=False)\n",
    "                log_success(f\"Saved compiled report to: {compiled_report_path}\")\n",
    "            except Exception as e_save_all:\n",
    "                log_error(f\"Failed to save compiled report {compiled_report_path}: {e_save_all}\")\n",
    "        else:\n",
    "            log_warning(\"No student data was collected to compile a report.\")\n",
    "\n",
    "        await browser_manager.close()\n",
    "        log_info(\"--- Finished Part 1: Data Extraction. Browser closed. ---\")\n",
    "\n",
    "        # Part 2: Analysis and CSV Generation\n",
    "        # This part runs after the browser is closed and the JSON report is (presumably) generated.\n",
    "        log_info(\"--- Starting Part 2: Submission Analysis and CSV Generation ---\")\n",
    "        # Ensure the model is available here. You might re-initialize if needed,\n",
    "        # or ensure the instance from the auth part is passed correctly if you structure it differently.\n",
    "        # For simplicity, using the global `model_for_auth_and_analysis` here.\n",
    "        await run_submission_analysis(llm_instance=model)\n",
    "        log_info(\"--- Finished Part 2: Submission Analysis and CSV Generation ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Run the Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the main function\n",
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        asyncio.run(main())\n",
    "    except Exception as e_main_run:\n",
    "        log_error(f\"Critical error in main execution: {e_main_run}\")\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Optional: Process Existing Data\n",
    "\n",
    "If you already have extracted data and just want to run the analysis part, you can use this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model for summarization\n",
    "analysis_model = ChatGoogleGenerativeAI(model='gemini-2.0-flash-lite')\n",
    "\n",
    "# Run just the analysis part (if you already have JSON data)\n",
    "async def run_just_analysis():\n",
    "    await run_submission_analysis(llm_instance=analysis_model)\n",
    "    \n",
    "# Uncomment and run the cell below to process existing data\n",
    "# asyncio.run(run_just_analysis())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}